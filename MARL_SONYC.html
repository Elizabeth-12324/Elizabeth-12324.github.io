<!DOCTYPE html>
<html lang="en-US" dir="ltr">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!--
    Document Title
    =============================================
    -->
    <title>SONYC Internship</title>
    <!--
    Favicons
    =============================================
    -->
    <link rel="apple-touch-icon" sizes="57x57" href="assets/images/favicons/3d_icon.png">
    <link rel="apple-touch-icon" sizes="60x60" href="assets/images/favicons/3d_icon.png">
    <link rel="apple-touch-icon" sizes="72x72" href="assets/images/favicons/3d_icon.png">
    <link rel="apple-touch-icon" sizes="76x76" href="assets/images/favicons/3d_icon.png">
    <link rel="apple-touch-icon" sizes="114x114" href="assets/images/favicons/3d_icon.png">
    <link rel="apple-touch-icon" sizes="120x120" href="assets/images/favicons/3d_icon.png">
    <link rel="apple-touch-icon" sizes="144x144" href="assets/images/favicons/3d_icon.png">
    <link rel="apple-touch-icon" sizes="152x152" href="assets/images/favicons/3d_icon.png">
    <link rel="apple-touch-icon" sizes="180x180" href="assets/images/favicons/3d_icon.png">
    <link rel="icon" type="image/png" sizes="192x192" href="assets/images/favicons/3d_icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="assets/images/favicons/3d_icon.png">
    <link rel="icon" type="image/png" sizes="96x96" href="assets/images/favicons/3d_icon.png">
    <link rel="icon" type="image/png" sizes="16x16" href="assets/images/favicons/3d_icon.png">
    <link rel="manifest" href="/manifest.json">
    <meta name="msapplication-TileColor" content="#ffffff">
    <meta name="msapplication-TileImage" content="assets/images/favicons/3d_icon.png">
    <meta name="theme-color" content="#ffffff">


    <!--
    Stylesheets
    =============================================

    -->
    <!-- Default stylesheets-->
    <link href="assets/lib/bootstrap/dist/css/bootstrap.min.css" rel="stylesheet">
    <!-- Template specific stylesheets-->
    <link href="https://fonts.googleapis.com/css?family=Roboto+Condensed:400,700" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Volkhov:400i" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,400,600,700,800" rel="stylesheet">
    <link href="assets/lib/animate.css/animate.css" rel="stylesheet">
    <link href="assets/lib/components-font-awesome/css/font-awesome.min.css" rel="stylesheet">
    <link href="assets/lib/et-line-font/et-line-font.css" rel="stylesheet">
    <link href="assets/lib/flexslider/flexslider.css" rel="stylesheet">
    <link href="assets/lib/owl.carousel/dist/assets/owl.carousel.min.css" rel="stylesheet">
    <link href="assets/lib/owl.carousel/dist/assets/owl.theme.default.min.css" rel="stylesheet">
    <link href="assets/lib/magnific-popup/dist/magnific-popup.css" rel="stylesheet">
    <link href="assets/lib/simple-text-rotator/simpletextrotator.css" rel="stylesheet">
    <!-- Main stylesheet and color file-->
    <link href="assets/css/style.css" rel="stylesheet">
    <link id="color-scheme" href="assets/css/colors/default.css" rel="stylesheet">
  </head>
  <body data-spy="scroll" data-target=".onpage-navigation" data-offset="60">
    <main>
      <div class="page-loader">
        <div class="loader">Loading...</div>
      </div>
      <nav class="navbar navbar-custom navbar-fixed-top" role="navigation">
        <div class="container">
          <div class="navbar-header">

            <!-- change the hfref to go to future about page instead -->

            <button class="navbar-toggle" type="button" data-toggle="collapse" data-target="#custom-collapse"><span class="sr-only">Toggle navigation</span><span class="icon-bar"></span><span class="icon-bar"></span><span class="icon-bar"></span></button><a class="navbar-brand" href="index.html">Elizabeth Mendoza</a>
          </div>
          <div class="collapse navbar-collapse" id="custom-collapse">
            <ul class="nav navbar-nav navbar-right">


              <li class="dropdown"><a class="dropdown" href="Resume.html">Resume</a>
                  </li>
                  <li><a href="about3.html">About</a></li>
                </ul>
              </li>
              </li>
            </ul>
          </div>
        </div>
      </nav>




      <!--HERE IS WHERE THE MAIN CHUNK OF THE PAGE BEGINS -->

      <div class="main">
        <section class="module bg-dark-30 portfolio-page-header" data-background="assets/images/SONYC_banner.jpg">
          <div class="container">
            <div class="row">
              <div class="col-sm-6 col-sm-offset-3">
                <h2 class="module-title font-alt">Music and Audio Research Lab: Machine Learning Internship</h2>
                <div class="module-subtitle font-serif"></div>
              </div>
            </div>
          </div>
        </section>

        <section class="module">
          <div class="container">
            <div class="row">
              <div class="col-sm-6 col-md-8 col-lg-8">

                <div class="post-images-slider">
                  <ul class="slides">
                    <img src="assets/images/SONYC.jpg" alt="Initial Theme"/>

                  </ul>
                </div>



              </div>
              <div class="col-sm-6 col-md-4 col-lg-4">
                <div class="work-details">
                  <h5 class="work-details-title font-alt">Project Summary</h5>
                  <p> During 2021-2022, I returned as a research intern at the NYU Music and Audio Research Lab. My internship focused on studying the effect of lossy-based MP3 compression on the performance of machine learning recognition models.</p>
            <!--      <p> <u><a href="https://wp.nyu.edu/birdvox/">BirdVox </a></u> is a collaboration between the Cornell Lab of Ornithology and NYU MARL that aims to use machine learning models in order to monitor bird migration patterns.</p>  -->
                  <p> In order to do so, I generated various Python scripts in order to separate, compress, and generate embeddings based on a pre-existing SONYC dataset. Afterwards, I used these embeddings to train various ML models at various compressions, and tested how well they could classify various sounds.  </p>


                  <ul>
                <!--    <li><strong>Skills: </strong><span class="font-serif">Unity, Maya</span> -->
                    </li>
                    <li><strong>Software: </strong><span class="font-serif">Python - KERAS, OpenL3, Spotify Pedalboard</span>
                    <li><strong>Date: </strong><span class="font-serif">November 2021 - September 2022</span>
                    </li>
                  </ul>

                </div>
              </div>
            </div>

          </div>

        </section>

        <section class="module">
          <div class="container">
            <div class="row">

              <!-- <div class="col-sm-6 col-md-4 col-lg-4"> -->
              <div class="col-sm-4 col-md-6 col-lg-6">
                <div class="work-details">
                  <h5 class="work-details-title font-alt">Project Breakdown</h5>

<!--
                <div class="post-images-slider">
                  <ul class="slides">
                    <li><img class="center-block" src="assets/images/2020Outreach.png" alt="Slider Image"/></li>
                    <li><img class="center-block" src="assets/images/2021Outreach.png" alt="Slider Image"/></li>
                    <li><img class="center-block" src="assets/images/2022Outreach.png" alt="Slider Image"/></li>
                  </ul>
                </div>
-->
                 <p>SONYC (Sounds of New York City) deploys acoustic recording devices to track noise pollution across NYC and often relies on a combination of expert, and crowdsourced volunteers in order to categorize and annotate the collected audio.</p>
                 <p>These annotations are incredibly time consuming and costly to produce, and so in response, MARL wants to implement machine learning audio recorders that could be able to identify and correctly classify the type of noise it is hearing. However, given that hours of audio are recorded at a time, storage quickly becomes an issue as they are currently recorded in WAV format which allows for lots of detail, but less recording time. </p>
                 <p>The compromise would be using compressed MP3 files in order to limit the storage size at the cost of losing audio quality. This is why my internship aimed to explore the possibilities of using compressed MP3 audio to train and test machine learning models to analyze how lossy-based compression impacts its performance. </p>
            </div>
            </div>

            <div class="col-sm-8 col-md-6 col-lg-6">
              <br></br>
              <img src="assets/images/MP3_chart.png" alt="Initial Theme"/>
              </div>
            </div>


          </div>
        </section>



          <section class="module">
            <div class="container">
              <div class="row">
                        <h5 class="work-details-title font-alt">Stratifying the Data</h5>
                        <p>The project began with organizing our data in order to ensure that our machine model was being trained with a distribution that would reflect real world acoustic environments.</p>
                        <p>The SONYC dataset used was split into 8 different categories: engine’s, machinery-impact, non-machinery-impact, powered-saw, alert-signal, music, human-voice, and dog sounds. Each audio clip recorded was around 10 seconds long, and guaranteed at least one of these sounds present.</p>
                        <p>In order to ensure that the model could correctly identify each category without any overlap, I wrote a python script to filter out any audios that contained more than one type of sound, as well as focusing on annotations made by an expert to ensure there wouldn’t be conflicting data.</p>
                        <p>From here, I separated the remaining audio into both training and testing datasets with a similar breakdown between the two to ensure there were similar real-world distributions in both. </p>
                        <img src="assets/images/SONYC_charts.png" alt="Data breakdown"/>

                  </div>
                </div>
                </section>

  <!--
                <section class="module">
                  <div class="container">
                    <div class="row">


                      <div class="col-sm-4 col-md-6 col-lg-6">
                        <div class="work-details">
                          <h5 class="work-details-title font-alt">Sampled SNR & Generating Embeddings </h5>
                          <p>Following this, I ran a script that would determine the highest SNR within each audio clip and then trim the clip to within a seconds range from the loudest moment. The reason for doing this was to cut down on the time it took to generate the embeddings, as well as ensuring that the model was properly trained to identify said sound. </p>
                          <p>Then, using OpenL3, I ran all of these trimmed audios through to create the embeddings and exported them as a CSV file to be read by the model later on. </p>
                      </div>
                    </div>

                    <div class="col-sm-8 col-md-6 col-lg-6">
                      <h5 class="work-details-title font-alt">Creating Compressed Datasets</h5>
                      <p>Using Spotify’s Pedalboard library, I wrote a script to then create compressed MP3 versions of the original filtered dataset. Pedalboard uses a lossy based algorithm to compress audio on various levels between 1-9 (9 being the highest level of compression).</p>
                      <p>With this, I generated four compressed datasets at levels 3, 5 , 7 and 9 and separated them into testing, training, and validation subsets based on the distribution of the original dataset.</p>

                      </div>
                    </div>


                  </div>
                </section>
              -->


                <section class="module">
                  <div class="container">
                    <div class="row">
                              <h5 class="work-details-title font-alt">Sampled SNR & Generating Embeddings </h5>
                              <p>Following this, I ran a script that would determine the highest SNR within each audio clip and then trim the clip to within a seconds range from the loudest moment. The reason for doing this was to cut down on the time it took to generate the embeddings, as well as ensuring that the model was properly trained to identify said sound. </p>
                              <p>Then, using OpenL3, I ran all of these trimmed audios through to create the embeddings and exported them as a CSV file to be read by the model later on. </p>

                        </div>
                      </div>
                      </section>

                      <section class="module">
                        <div class="container">
                          <div class="row">
                                    <h5 class="work-details-title font-alt">Creating Compressed Datasets</h5>
                                    <p>Using Spotify’s Pedalboard library, I wrote a script to then create compressed MP3 versions of the original filtered dataset. Pedalboard uses a lossy based algorithm to compress audio on various levels between 1-9 (9 being the highest level of compression).</p>
                                    <p>With this, I generated four compressed datasets at levels 3, 5 , 7 and 9 and separated them into testing, training, and validation subsets based on the distribution of the original dataset. To get a visual representation of the way certain classes changed after being compressed, I created spectograms to help identify a potential breaking point for the model. </p>

                                    <div class="post-images-slider">
                                      <ul class="slides">
                                            <li><img class="center-block" src="assets/images/12.png" alt="Slider Image"/></li>
                                            <li><img class="center-block" src="assets/images/13.png" alt="Slider Image"/></li>
                                            <li><img class="center-block" src="assets/images/14.png" alt="Slider Image"/></li>
                                        </ul>
                                      </div>
                                    <!--
                                    <img src="assets/images/11_1.png" alt="Initial Theme"/>
                                  -->
                              </div>
                            </div>
                            </section>



                            <section class="module">
                              <div class="container">
                                <div class="row">
                                          <h5 class="work-details-title font-alt">Running the Model</h5>
                                          <p>Using KERAS, I trained a machine learning model using the training data of the zero compression dataset, and would run it against the embeddings of the testing dataset at various compression levels in order to see the cutoff point for the model's accuracy. </p>
                                          <p>Following this, I then trained two separate model’s using the level 3 and level 5 compressed training data and tested it against the various compression levels as well, giving us a total of three separate models. When tested on their respective compressions (ex: trained with level 0/tested with level 0, trained with level 3/tested with level 3, etc), the models averaged as follows: <p>


                                          <b>
                                          <p>Original Compression- Train Accuracy: 72.91% | Val Accuracy: 59.87% </p>
                                          <p>Compression Lvl 3- Train Accuracy: 67.49% | Val Accuracy: 56.58% </p>
                                          <p>Compression Lvl5- Train Accuracy: 70.28% | Val Accuracy: 51.32% </p>
                                        </b>
                                    </div>
                                  </div>
                                  </section>


                  <section class="module">
                    <div class="container">
                      <div class="row">
                                <h5 class="work-details-title font-alt">Final Results</h5>


                                <p>After running all of the models, I measured their performance by creating a confusion matrix with the true positives, false positives, false negatives, precision, recall and the F1 accuracy values. </p>
                                <div class="post-images-slider">
                                  <ul class="slides">
                                        <li><img class="center-block" src="assets/images/5.png" alt="Slider Image"/></li>
                                        <li><img class="center-block" src="assets/images/6.png" alt="Slider Image"/></li>
                                        <li><img class="center-block" src="assets/images/7.png" alt="Slider Image"/></li>
                                    </ul>
                                  </div>
                                <p>The F1 values were then graphed and we could compare how each model performed in certain categories.</p>
                                <img src="assets/images/8.png" alt="Initial Theme"/>
                                <img src="assets/images/9.png" alt="Initial Theme"/>
                                <img src="assets/images/10.png" alt="Initial Theme"/>
                                <p>Unsurprisingly, the original trained dataset performed best, and would usually lower in accuracy as the compression levels raised. However, there were certain categories that would actually perform better when compressed, leading us to wonder whether this is due to the type of sound/duration, or a disbalance in testing data. </p>

                          </div>
                        </div>
                      </section>

          <section class="module">
            <div class="container">
              <div class="row">

                <!-- <div class="col-sm-6 col-md-4 col-lg-4"> -->
                  <div class="work-details">
                    <h5 class="work-details-title font-alt">Reflection</h5>
                    <p>This experience was one that taught me a great amount about machine learning while building off of my previous experience working within the field of audio research. </p>
                    <p>Looking at further avenues of exploration, its worth mentioning that some of the results could have been impacted by the step in which we sample the highest SNR in each audio clip. The assumption being that, the loudest SNR would mean that is the most likey point to contain the classified sound. However, this is not a guarantee and it could result in the model incorrectly learning what certain categories sound like. </p>
                  </div>
              </div>

            </div>
          </section>


          <section class="module-small bg-dark">
            <div class="container">
              <div class="row">
                <div class="col-sm-6 col-sm-offset-3 text-center">
                  <h4 class="font-alt mb-20">Contact Me:</h4>
                  <h4 class="font-alt mb-20">emendozamedia@gmail.com</h4>
                </div>
              </div>
            </div>
          </section>

          <hr class="divider-d">
          <footer class="footer bg-dark">
            <!--
            <div class="container">
              <div class="row">
                <div class="col-sm-6">
                  <p class="copyright font-alt">&copy; 2017&nbsp;<a href="index.html">TitaN</a>, All Rights Reserved</p>
                </div>
                <div class="col-sm-6">
                  <div class="footer-social-links"><a href="#"><i class="fa fa-facebook"></i></a><a href="#"><i class="fa fa-twitter"></i></a><a href="#"><i class="fa fa-dribbble"></i></a><a href="#"><i class="fa fa-skype"></i></a>
                  </div>
                </div>
              </div>
            </div>
          -->
          </footer>
          <div class="scroll-up"><a href="#totop"><i class="fa fa-angle-double-up"></i></a></div>
        </div>
      </main>
      <!--
      JavaScripts
      =============================================
      -->
      <script src="assets/lib/jquery/dist/jquery.js"></script>
      <script src="assets/lib/bootstrap/dist/js/bootstrap.min.js"></script>
      <script src="assets/lib/wow/dist/wow.js"></script>
      <script src="assets/lib/jquery.mb.ytplayer/dist/jquery.mb.YTPlayer.js"></script>
      <script src="assets/lib/isotope/dist/isotope.pkgd.js"></script>
      <script src="assets/lib/imagesloaded/imagesloaded.pkgd.js"></script>
      <script src="assets/lib/flexslider/jquery.flexslider.js"></script>
      <script src="assets/lib/owl.carousel/dist/owl.carousel.min.js"></script>
      <script src="assets/lib/smoothscroll.js"></script>
      <script src="assets/lib/magnific-popup/dist/jquery.magnific-popup.js"></script>
      <script src="assets/lib/simple-text-rotator/jquery.simple-text-rotator.min.js"></script>
      <script src="assets/js/plugins.js"></script>
      <script src="assets/js/main.js"></script>
      </body>
      </html>
